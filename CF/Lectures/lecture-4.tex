\chapter{Lecture 4: 25/09}

\section{Subordinator process}
In this lecture we will talk about the subordinator process. In very general 
terms a subordinator is a process that is increasing in time and that we use to
sub-index another process. In other words we use it to change the "speed" of the
flow information and of the process. We will see that the subordinator process
is a very useful tool in the study of L\'evy processes.

We will start by stating the formal definition of a subordinator process.

\begin{definition}[Subordinator process]
Let $X_t$ be a L\'evy process. We say that $X_t$ is a subordinator process if
and only if it satisfies all of the following conditions:

\begin{enumerate}[1)]
    \item $X_t \geq 0$ for all $t \geq 0$ almost surely.
    \item $\exists \bar{t} > 0$ such that $X_{\bar{t}} \geq 0$ almost surely.
    \item $X_t$ is not decreasing.
    \item Given that the L\'evy decomposition of $X_t$ is $(\gamma, A, \nu)$,
    then:
    \begin{itemize}
        \item $\nu((-\infty, 0)) = 0$. In other words, the L\'evy measure of
        $X_t$ is zero in the negative real line. This is equivalent to saying
        that the process $X_t$ does not have negative jumps.
        \item $A=0$. In other words, the process $X_t$ does not have a
        continuous (Brownian Motion) component.
        \item $b = \gamma - \int_{|x|\leq1} x \, \nu(dx) > 0$. In other words
        the drift of the process $X_t$ is non-negative. Recall that $\int_{
        |x|\leq1} x \, \nu(dx)$ is the compensator process.
        \item $\int_{|x|\leq 1} x \, \nu(dx) < + \infty$. In other words, the
        compensator process is finite.
    \end{itemize}
\end{enumerate}
\end{definition}

\begin{remark*}
Note that the fourth condition is equivalent to saying that $X_t$ is a
finite variation L\'evy process with and non-negative drift ($b > 0$) and
no negative jumps ($\nu((-\infty, 0)) = 0$).
\end{remark*}

\begin{theorem}
The conditions above are equivalent to one another.    
\end{theorem}

\begin{proof}
$ $ \\
\begin{itemize}
    \item $1) \implies 2)$: This is trivial. If $X_t \geq 0$ for all $t \geq
    0$ then $X_{\bar{t}} \geq 0$ for some $\bar{t} > 0$.
    \item $3) \implies 1)$: This is also trivial. By assumption $X_t$ is not
    decreasing and since it is L\'evy $X_0=0$ almost surely, then $X_t \geq 0$
    for all $t \geq 0$ almost surely.
    \item $2) \implies 3)$: By hypothesis of $2)$ we know that
    $\exists\bar{t}>0$ such that $X_{\bar{t}} \geq 0$ almost surely. Let us now
    take $n\in\N$, then we rewrite $X_{\bar{t}}$ as follows:
    \[ X_{\bar{t}} = \underbrace{X_{\bar{t}} - X_{\bar{t}\frac{(n-1)}{n}}}_{
         \geq 0} + \underbrace{X_{\bar{t}\frac{(n-1)}{n}} - X_{\bar{t}
        \frac{(n-2)}{n}}}_{\geq 0} + \dots + \underbrace{X_{\frac{2\bar{t}}{n}}
        - X_{\frac{\bar{t}}{n}}}_{\geq 0} + \cancelto{0}{X_0} \geq 0 \]
    Thus since by the assumption on $X_t$ being a L\'evy process we know that 
    the increments are iid, this becomes a sum of iid random variables, that is
    it is equal to $n$ times the same random variable. Furthermore since we
    know that $X_{\bar{t}} \geq 0$ almost surely, then we know that each
    increment must be non-negative. Hence:
    \[ \implies \left( X_{\bar{t} \frac{i}{n}} - X_{\bar{t} \frac{i-1}{n}}
        \right) \geq 0 \quad \forall i \in \{1, \dots, n\} \]
    This can easily extended to be of the form:
    \[ \left( X_{\bar{t}p} - X_{\bar{t}q} \right) \geq 0 \quad \forall p > q
        \in \Q \]
    Furthermore since the set $\Q$ is dense in the set of reals $\R$ this can be
    even further be extended to be of the form:
    \[ \left( X_t - X_s \right) \quad \forall t > s \in \R \]
    So far we have thus proved that the first 3 conditions are equivalent among
    each other, now we will pass on to prove that the fourth condition is
    equivalent to the third.
    \item $4) \implies 3)$: We can easily see that:
    \[ \int_{|x|\leq1} x \, \nu(dx) < +\infty \implies
    X_t = bt + A\cdot B_t + \sum_{s\in[0,t]} \Delta X_s \]
    Furthermore since we have that $\nu((-\infty, 0)) = 0$ then we know that
    $\Delta X_s \geq 0$ almost surely. Thus we can conclude that $X_t$ is not
    decreasing since it is a sum of non-negative random variables.
    \item $3) \implies 4)$: In order to prove this last result we need a
    further theorem.
    \begin{theorem*}
        Let $f$ be a C\'adl\'ag function and not-decreasing. Then $f$ is also
        of finite variation.
    \end{theorem*}
    Now, we also know that a L\'evy process that is of finite variation also has
    that:
    \[ X_t \text{ FV } \implies A=0, \, \int_{|x|\leq1} |x| \, \nu(dx) < +\infty
    \]
    These conditions thus lead us to conclude that:
    \[ X_t = bt + \sum_{s\in[0,t]} \Delta X_s \text{ is not decreasing} \iff
        \left\{ \begin{array}{l}
            b > 0 \\ \Delta X_s \geq 0 \text{ a.s.} \iff \nu((-\infty, 0)) = 0
        \end{array} \right.
    \]
\end{itemize}
\end{proof}

\section{How to construct a subordinator process}
We will now go on to see how a subordinator process can be constructed at will.

\begin{theorem}
Let $X_t$ be a L\'evy process in $\R^d$ (with $d\geq1$) with L\'evy-It\^o
decomposition $(\gamma, A, \nu)$. Let also $f:\R^d\mapsto\Rpos$ such that $f(x) = 
O(|x|^2)$ in a neighborhood of $0$. Then:
\[ S_t = \sum_{\begin{subarray}{c} s\in[0,t] \\ \Delta X_s \neq 0 \end{subarray}}
    f(\Delta X_s) \]
is a subordinator process.
\end{theorem}

Before prooving this theorem let us make a few remarks and observations:

\begin{remark*}
Note that in this theorem we have only used $\nu$ but have not used $A$ or
$\gamma$ at all.
\end{remark*}

\begin{remark}
    Let us note that from the L\'evy-Khintchine formula, we know that given 
    $(\gamma, A, \nu)$ we can easily write the characteristic exponent:
    \[ \Psi_{X_s}(z) = i\gamma^T \cdot z - \frac{1}{2} z^T A z + \int_{\R^d}
        \left( e^{iz^T x} - 1 - iz^T x \I_{|x|\leq1} \right) \nu(dx) \]
    Where we divide small (i.e. $|x|\leq1$) and large (i.e. $|x|>1$) jumps.
    Furthermore if we know that $\int_{|x|\leq1} |x| \, \nu(dx) < +\infty$ then:
    \[ \Psi_{X_s}(z) = i\gamma^T_c \cdot z - \frac{1}{2} z^T A z + \int_{\R^d}
        \left( e^{iz^T x} - 1 \right) \nu(dx) \]
    Where $\gamma_c$ is:
    \[ \gamma_c = \gamma - \int_{|x|\leq1} x \, \nu(dx) \]
    Or we could also write:
    \[ \Psi_{X_s}(z) = i\gamma^T_{\hat{C}} \cdot z - \frac{1}{2} z^T A z + \int_{\R^d}
        \left( e^{iz^T x} - 1 - iz^T x \right) \nu(dx) \]
    with the alternative choice of $\gamma_{\hat{C}}$:
    \[ \gamma_{\hat{C}} = \gamma + \int_{|x|>1} x \, \nu(dx) \]
    Let us notice that the $\int_{|x|\leq1} |x| \, \nu(dx)$ term only
    appears in the drift.
    It is rather easy to take a function $g:\R^d\mapsto\R$ such that:
    \begin{align*}
        g(x) &= O\left(|x|\right) \text{ for } |x|\to 0 \\
        g(x) &= O\left(\frac{1}{|x|}\right) \text{ for } |x|\to \infty
    \end{align*}
    Then we can write the characteristic exponent as:
    \begin{align*}
        \Psi_{X_s}(z) & = i\hat{\gamma}^T \cdot z - \frac{1}{2} z^T A z +
        \int_{\R^d} \left( e^{iz^T x} - 1 - iz^T x g(x) \right) \nu(dx) \\
        \text{ with } \hat\gamma & = \gamma - \int_{\R^d} x \left(\I_{|x|\leq1}
        + g(x)\right) \, \nu(dx)
    \end{align*}
    Thus we can freely choose $g$ as we wish and we will still have a valid
    characteristic exponent. This is a very useful property that we will use
    later on.
\end{remark}

Let us also see a few examples of $g$ that we can use:
\begin{itemize}
    \item $g(x) = \I_{|x|\leq1}$
    \item $g(x) = \I_{|x|\leq\epsilon}$ for some $\epsilon > 0$
\end{itemize}

Finally we move on to the proof of the theorem.

\begin{proof}
Since $f$ is a non-negative function, then we know that $S_t$ is non-decreasing.
All we need to prove is that $S_t$ is a L\'evy process. This is equivalent to
proving that $S_t$ respects the following properties:
\begin{itemize}
    \item $S_t$ is right-continuous.
    \item $S_t$ is left-limited.
    \item $S_0 = 0$.
    \item $S_t$ has independent increments.
    \item $S_t$ is stochastically continuous.
\end{itemize}

Recall that $S_t$ is defined as:
\[ S_t = \sum_{\begin{subarray}{c} s\in[0,t] \\ \Delta X_s \neq 0 \end{subarray}}
    f(\Delta X_s) \]
Where $X_t$ is a L\'evy process and $f$ is a non-negative function such that
$f(x) = O(|x|^2)$ in a neighborhood of $0$. 

From the definition, since $t$ is included into the sum, we can quite trivially
conclude that $S_t$ is right-continuous. Furthermore, by the same logic on the
definition, we can conclude that $S_0 = 0$.
It also has independent increments since it is a sum of independent increments
of $X_t$, and by the same logic it is also stochastically continuous.
What we really need to prove is that the process is left-limited.
This entails the two following properties:
\begin{align*}
    \int_{|x|>1} \, \nu_S(dx) & < +\infty \\
    \int_{|x|\leq1} |x|^2 \, \nu_S(dx) & < +\infty
\end{align*}
By the hypothesis that $X_t$ be a L\'evy process we know that:
\[ \int_{|x|\leq1} |x|^2 \, \nu_X(dx) < +\infty \]
And we want to pass to:
\[ \int_{|x|\leq1} |x|^2 \, \nu_S(dx) < +\infty \]
Which is also equivalent to asking that:
\[ \int_{|x|\leq\epsilon} |x|^2 \, \nu_S(dx) < +\infty \quad \forall \epsilon >
    0 \]
Again, from the hypothesis that $f(x) = O(|x|^2)$ in a neighborhood of $0$ we
have that:
\[ \forall C>0 \, \exists \epsilon > 0 \quad f(\Delta X_s) \leq C |\Delta X_s|^2
    \quad \text{if } |\Delta X_s| \leq \epsilon \]
This can be piecewise extended to write:
\[ \sum_{\begin{subarray}{c} s\leq t \\ 0 < |\Delta X_s| \leq \epsilon
\end{subarray}} f(\Delta X_s) \leq C \cdot \sum_{\begin{subarray}{c} s\leq t \\
0 < |\Delta X_s| \leq \epsilon \end{subarray}} |\Delta X_s|^2 \]
Now, we know that $X_t$ is a L\'evy process and thus that this second term is
bounded. Thus we can conclude that:

\[ 
    S_t = \sum_{\begin{subarray}{c} s\leq t \\ 0 < |\Delta X_s| \leq \epsilon
    \end{subarray}} f(\Delta X_s) + \sum_{\begin{subarray}{c} s\leq t \\ |\Delta
    X_s| > \epsilon \end{subarray}} f(\Delta X_s)
\]
And here we have that the first term is bounded as seen above, and the second
term is of finite cardinality, since we have a finite number of jumps that are
larger than $\epsilon$. Thus we can conclude that $S_t$ is left-limited and
therefore that it is a L\'evy process.
\end{proof}

\section{Kou Model: Matlab implementations}
We will now move on to write a Matlab function to simulate the Kou model.

We will start off by writing a function that simulates directly the L\'evy
process $X_t$. Recall that in the Kou model we have the
following definition:
\[ X_t = \gamma t + \sigma W_t + \sum_{i=1}^{N_t} Y_i \]
Where $Y_i$ is a jump of the form:
\[ Y_i = \left\{ \begin{array}{l} 
    \text{Exp}(\lambda^+) \text{ with probability } p \\
    \text{Exp}(\lambda^-) \text{ with probability } 1-p
\end{array} \right. \]
Hence the increase in the process is given by:
\[ \Delta X_t = X_{(i+1)\Delta t} - X_{i\Delta t} = \gamma \Delta t + \sigma
    (\underbrace{W_{(i+1) \Delta t} - W_{i\Delta t}}_{
    \sim \mathcal{N}(0, \Delta t)}) + \sum_{\begin{subarray}{c} s\in[i\Delta t,
    (i+1)\Delta t] \\ Y_s \neq 0 \end{subarray}} Y_s \]
So, in our function we will need to have the following inputs:
\begin{itemize}
    \item $N_{sim}$: the number of simulations to run.
    \item $T$: the time horizon.
    \item $M$: the number of time steps.
    \item Model parameters: $\sigma, \lambda, \lambda^+, \lambda^-, p$.
\end{itemize}

Furthermore, recall that we are working under the risk-neutral measure and that
$X_t$ is modeling the exponent of the stock price $S_t = S_0e^{rt+X_t}$. Thus
we will have:
\[ \E^\Q[S_te^{-rT}] = S_0 \implies S_0 \E[e^{X_t}] = S_0 \implies \E[e^{X_t}]
    = 1 \iff \Phi_{X_t}(-i) = 1 \iff \Psi_{X_t}(-i) = 0 \]

For the Kou process the characteristic exponent is given by:
\begin{align*}
    \Psi_{X_t}(u) &= -\frac{\sigma^2u^2}{2} + i\gamma u + iu\lambda + iu\lambda
    \left( \frac{p}{\lambda^+-iu} - \frac{1-p}{\lambda^-+iu}\right) \\
    \implies \Psi_{X_t}(-i) &= \frac{\sigma^2}{2} + \gamma - \lambda \left(
    \frac{p}{\lambda^++1} - \frac{1-p}{\lambda^-+1}\right) = 0 \\
    \implies \gamma &= \lambda \left( \frac{p}{\lambda^++1} -
    \frac{1-p}{\lambda^-+1}\right) - \frac{\sigma^2}{2}
\end{align*}

Thus after we can compute $\gamma$ like so in order to obey the assumption of
working under the risk-neutral measure.

For the actual simulation, we will sample the number of jumps for each simulation
from a Poisson distribution with parameter $\lambda T$. Then we will sample the
continuous component and add the jumps. The code is as follows:

\begin{minted}[linenos]{matlab}
function X=Kou_Simulation(Nsim,T,params,M)

% get the fuction and model parameters
dt=T/M;
X=zeros(Nsim,M+1);
sigma=params(1);
lambda=params(2);
p=params(3);
lambdap=params(4);
lambdam=params(5);
% characteristic exponent (equivalent to the above)
Psi=@(u) -sigma^2*u^2/2+1i*u*lambda*...
    (p/(lambdap-1i*u)-(1-p)/(lambdam+1i*u));
drift=-Psi(-1i);

% Sample number of jumps for each simulation
NT = icdf('Poisson',rand(Nsim,1),lambda*T);
% Sample Continuous Component and Add the Jumps
Z=randn(Nsim,M);
for j=1:Nsim % simulation loop
    % sample NT(j) jump times from a uniform distribution from 0 to T
    Jump_Times=sort( rand(1,NT(j)) )*T;
    for i=1:M % time loop
        % continuous part (adjust the standard deviation and add the drift)
        X(j,i+1)=X(j,i)+drift*dt+sigma*sqrt(dt)*Z(j,i);
        % add the jump component
        for jj=1:NT(j) 
            % if the jump time is in the interval [i*dt,(i+1)*dt]
            if Jump_Times(jj)>(i-1)*dt &&...
                    Jump_Times(jj)<=i*dt
                % sample the jump size
                if rand<p % positive jump
                    Y=icdf('Exponential',rand,1/lambdap);
                else % negative jump
                    Y=-icdf('Exponential',rand,1/lambdam);
                end
                % add jump
                X(j,i+1)=X(j,i+1)+Y;
            end
        end
    end
end
\end{minted}

This is a slightly inefficient but very intuitive way of implementing the Kou
model. We will now see a more efficient way of implementing the model by 
refactoring the loops.
Indeed we can see:
\[ \sum_{\begin{subarray}{c} s\in[i\Delta t, (i+1)\Delta t] \\ Y_s \neq 0
\end{subarray}} Y_s \sim \sum_{\begin{subarray}{c} s\in[0, \Delta t] \\ Y_s \neq
0 \end{subarray}} Y_s \]

\begin{minted}[linenos]{matlab}
function X=Kou_Simulation_v2(Nsim,T,params,M)

%% same code as above ...

% Sample number of Jumps in each time interval
Ndt=icdf('Poisson',rand(Nsim,M),lambda*dt);
% Sample Continuous Component and Add the Jumps
Z=randn(Nsim,M);
for i=1:M % time loop
   % continuous part (matrix multiplication)
   X(:,i+1)=X(:,i)+drift*dt+sigma*sqrt(dt)*Z(:,i);
   for j=1:Nsim % simulation loop
       for jj=1:Ndt(j,i)
           % sample the jump size
           if rand<p % positive jump
               Y=icdf('Exponential',rand,1/lambdap);
           else % negative jump
               Y=-icdf('Exponential',rand,1/lambdam);
           end
           % add jump
           X(j,i+1)=X(j,i+1)+Y;
       end
   end
end
\end{minted}

Here we are directly sampling the number of jumps in each time interval rather
than in the whole time horizon. This relieves us from having to check whether
the jump time is in the interval $[i\Delta t, (i+1)\Delta t]$.

\textbf{N.B.} if $N_{\Delta t}(j,i) = 0$ then the loop will not be executed at
all.

\miss{There was a third implementation of the Kou model that the professor
did show but did not upload to Webeep.}
